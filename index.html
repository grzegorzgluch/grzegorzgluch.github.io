<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>GG</title>
        <link rel="icon" type="image/x-icon" href="dist/assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="dist/css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">GRZEGORZ G≈ÅUCH</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="dist/assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#teaching">Teaching</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0" style="font-size: 45px;">
                        Grzegorz (Greg)
                        <span class="text-primary">G≈Çuch</span>
                    </h1>
                    <div class="subheading mb-5" style="font-size: 18px;">
                        Simons Institute for the Theory of Computing, Melvin Calvin Laboratory, Room 318 ¬∑ Berkeley ¬∑ CA 94720
                    </div>
                    <p class="lead mb-5">I am a <a href="https://simons.berkeley.edu/people/postdoctoral-researchers" target="_blank">Simons-Berkeley Postdoctoral Researcher</a> hosted by <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/goldwasser.html" target="_blank">Shafi Goldwasser</a>.
                      I completed my PhD at EPFL, where I was advised by a duo: <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> and
                      <a href="https://theory.epfl.ch/kapralov/" target="_blank">Michael Kapralov</a>. Before that
                    I studied at University of Wroclaw working with <a href="https://ii.uni.wroc.pl/~jma/index.phtml" target="_blank">Jerzy Marcinkowski</a>.</p>
                    <p class="lead mb-5">I work on the theoretical aspects of AI, focusing on AI safety and resilience. I am a member of the <a href='https://simons.berkeley.edu/research-pods/resilience-research-pod' target='_blank'>Resilience Research Pod at Simons</a>. My research interests also include quantum complexity theory and its connections to physics.</p>
                    <p class="lead mb-5">Trying to make an impact.</p>
                    <p class="lead mb-5">Contact: grzegorzgluch93@gmail.com</p>
                    <div class="social-icons">
                        <!--<a class="social-icon" href="https://scholar.google.com/citations?user=gg2sjRIAAAAJ&hl=en" target="_blank"><i class="fab fa-google"></i></a>-->
                        <!-- <a class="social-icon" href="https://www.linkedin.com/in/grzegorz-g%C5%82uch-58811b113/" target="_blank"><i class="fab fa-linkedin-in"></i></a> -->
                        <a href="https://scholar.google.com/citations?user=gg2sjRIAAAAJ&hl=en" target="_blank"><img src="dist/assets/img/google2.png" alt="HTML tutorial" style="width:70px;height:70px;"></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <p class="lead mb-5">Authors appear in alphabetical order (a convention in theoretical computer science).</p>

                    <style>
                        .btn-group .btn:not(:last-child) {
                            margin-right: 8px;
                        }
                    </style>
                    <!--
                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://eccc.weizmann.ac.il/report/2021/016/#revision1" style="text-decoration: none"><h4 class="mb-0">What fools you, makes you stronger</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://theory.epfl.ch/mika/">Mika G√∂√∂s</a>, Kaspars Balodis,
                                <a href="https://cs.uwaterloo.ca/people-profiles/shalev-ben-david">Shalev Ben-David</a>, <a href="http://www.robinkothari.com/">Robin Kothari</a></div>
                            <div class="subsubheading mb-0"><b>FOCS 2021</b> <span class="badge badge-success badge-pill">Invited to SICOMP Special Issue</span> <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://doi.org/10.1109/FOCS52979.2021.00020" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://eccc.weizmann.ac.il/report/2021/016/#revision1" class="btn btn-secondary" role="button" aria-pressed="true">eccc</a>
                                    <a href="https://arxiv.org/abs/2102.08348" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                    <a href="https://www.youtube.com/watch?v=VkxQa4EOsWY&list=PLXrDGj9O32r0ARRZQRRlqFYz6SuKKtbrb" class="btn btn-secondary" role="button" aria-pressed="true">FOCS talk</a>
                                </div>
                                <details><summary>Abstract (Informal)</summary> We exhibit an unambiguous k-DNF formula that requires CNF width almost k^2, which is optimal up to logarithmic factors.
                                    As a consequence, we get a near-optimal solution to the Alon--Saks--Seymour problem in graph theory (posed in 1991), which asks: How large a gap can there
                                    be between the chromatic number of a graph and its biclique partition number? Our result is also known to imply several other improved separations in
                                    query and communication complexity.</details>

                            </div>
                        </div>
                    </div>
                    -->

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2507.07341" target="_blank" style="text-decoration: none"><h4 class="mb-0">On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://zuseschoolrelai.de/people/scientists/sarah-ball/" target="_blank">Sarah Ball</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/goldwasser.html" target="_blank">Shafi Goldwasser</a>, <a href="https://jpsm.umd.edu/facultyprofile/kreuter/frauke" target="_blank">Frauke Kreuter</a>, <a href="https://omereingold.wordpress.com/" target="_blank">Omer Reingold</a>, <a href="https://guyrothblum.wordpress.com/" target="_blank">Guy N. Rothblum</a> </div>
                                <div class="subsubheading mb-0"><b>Preprint</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://arxiv.org/abs/2507.07341" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary>
                                  With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. Our main results demonstrate computational challenges in filtering both prompts and outputs. First, we show that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. Our second main result identifies a natural setting in which output filtering is computationally intractable. All of our separation results are under cryptographic hardness assumptions. In addition to these core findings, we also formalize and study relaxed mitigation approaches, demonstrating further computational barriers. We conclude that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on our technical results, we argue that an aligned AI system's intelligence cannot be separated from its judgment.</details>
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2112.09625" target="_blank" style="text-decoration: none"><h4 class="mb-0">A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/goldwasser.html" target="_blank">Shafi Goldwasser</a> </div>
                                <div class="subsubheading mb-0"><b>Preprint</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://arxiv.org/abs/2504.20310" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary>
                                  In this paper, we initiate a cryptographically inspired theoretical study of detection versus mitigation of adversarial inputs produced by attackers of Machine Learning algorithms during inference time.
                                    We formally define defense by detection (DbD) and defense by mitigation (DbM). Our definitions come in the form of a 3-round protocol between two resource-bounded parties: a trainer/defender and an attacker. The attacker aims to produce inference-time inputs that fool the training algorithm. We define correctness, completeness, and soundness properties to capture successful defense at inference time while not degrading (too much) the performance of the algorithm on inputs from the training distribution.
                                    We first show that achieving DbD and achieving DbM are equivalent for ML classification tasks. Surprisingly, this is not the case for ML generative learning tasks, where there are many possible correct outputs that can be generated for each input. We show a separation between DbD and DbM by exhibiting a generative learning task for which is possible to defend by mitigation but is provably impossible to defend by detection under the assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE), publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation phase uses significantly fewer samples than the initial training algorithm.
                                </details>
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2112.09625" target="_blank" style="text-decoration: none"><h4 class="mb-0">The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://b-turan.github.io/" target="_blank">Berkant Turan</a>, <a href="https://sites.google.com/view/sgnagarajan/home" target="_blank">Sai Ganesh Nagarajan</a>, <a href="https://www.pokutta.com/" target="_blank">Sebastian Pokutta</a> </div>
                                <div class="subsubheading mb-0"><b>NeurIPS 2025, Theoretical Foundations of Foundation Models @ ICML 2024</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://arxiv.org/abs/2410.08864" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary>
                                We formalize and analyze the trade-off between backdoor-based watermarks and adversarial defenses, framing it as an interactive protocol between a verifier and a prover. 
                                    While previous works have primarily focused on this trade-off, our analysis extends it by identifying transferable attacks as a third, counterintuitive but necessary option. 
                                    Our main result shows that for all learning tasks, at least one of the three exists: a watermark, an adversarial defense, or a transferable attack. 
                                    By transferable attack, we refer to an efficient algorithm that generates queries indistinguishable from the data distribution and capable of fooling all efficient defenders. 
                                    Using cryptographic techniques, specifically fully homomorphic encryption, we construct a transferable attack and prove its necessity in this trade-off. 
                                    Furthermore, we show that any task that satisfies our notion of a transferable attack implies a cryptographic primitive, thus requiring the underlying task to be computationally complex. 
                                    Finally, we show that tasks of bounded VC-dimension allow adversarial defenses against all attackers, while a subclass allows watermarks secure against fast adversaries.                               
                                </details>
                            </div>
                        </div>
                    </div>
                    
                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2112.09625" target="_blank" style="text-decoration: none"><h4 class="mb-0">Nonlocality under Computational Assumptions</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://lasec.epfl.ch/people/barooti/" target="_blank">Khashayar Barooti</a>, <a href="https://agheorghiu.com/" target="_blank">Alexandru Gheorghiu</a>, <a href="https://scholar.google.com/citations?user=u-DfxLcAAAAJ&hl=fr" target="_blank">Marc-Olivier Renou</a> </div>
                                <div class="subsubheading mb-0"><b>STOC 2024, QIP 2025</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://dl.acm.org/doi/abs/10.1145/3618260.3649750" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/2303.02080" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary>
                                 Nonlocality and its connections to entanglement are fundamental features of quantum mechanics that have found numerous applications in quantum information science. A set of correlations is said to be nonlocal if it cannot be reproduced by spacelike-separated parties sharing randomness and performing local operations. An important practical consideration is that the runtime of the parties has to be shorter than the time it takes light to travel between them. One way to model this restriction is to assume that the parties are computationally bounded. We therefore initiate the study of nonlocality under computational assumptions and derive the following results:
                                (a) We define the set ùñ≠ùñæùñ´ (not-efficiently-local) as consisting of all bipartite states whose correlations arising from local measurements cannot be reproduced with shared randomness and polynomial-time local operations.
                                (b) Under the assumption that the Learning With Errors problem cannot be solved in quantum polynomial-time, we show that ùñ≠ùñæùñ´=ùñ§ùñ≠ùñ≥, where ùñ§ùñ≠ùñ≥ is the set of all bipartite entangled states (pure and mixed). This is in contrast to the standard notion of nonlocality where it is known that some entangled states, e.g. Werner states, are local. In essence, we show that there exist (efficient) local measurements producing correlations that cannot be reproduced through shared randomness and quantum polynomial-time computation.
                                (c) We prove that if ùñ≠ùñæùñ´=ùñ§ùñ≠ùñ≥ unconditionally, then ùô±ùöÄùôø‚â†ùôøùôø. In other words, the ability to certify all bipartite entangled states against computationally bounded adversaries gives a non-trivial separation of complexity classes.
                                (d) Using (c), we show that a certain natural class of 1-round delegated quantum computation protocols that are sound against ùôøùôø provers cannot exist.
                                </details>
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2112.09625" target="_blank" style="text-decoration: none"><h4 class="mb-0">Bayes Complexity of Learners vs Overfitting</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> </div>
                                <div class="subsubheading mb-0"><b>Under Review</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://arxiv.org/abs/2303.07874" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary>
                                  We introduce a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks and linear schemes. While there is a large set of papers which describes bounds that have each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers.
                                  Even though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. An upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and 4-layer neural networks for periodic functions.
                               </details>
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2112.09625" target="_blank" style="text-decoration: none"><h4 class="mb-0">Breaking a Classical Barrier for Classifying Arbitrary Test Examples in the Quantum Model</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://lasec.epfl.ch/people/barooti/" target="_blank">Khashayar Barooti</a>, <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> </div>
                                <div class="subsubheading mb-0"><b>AISTATS 2023 (Poster at QIP 2022)</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://proceedings.mlr.press/v206/gluch23a.html" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/2112.09625" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Informal Abstract</summary>
                                This paper presents a surprising connection between recent advances in delegation of quantum computation to adversarial robustness.
                                Imagine an adversary A that is in between a classifier C and a source S producing samples to be classified. Can we design an interactive protocol
                                 between C and the A, in which A would convince C that the samples being sent for classification really came from S? Using ideas from delegation
                                 of quantum computation we show it is possible to do it if A receives quantum states from S and A communicates with C classically. Why do we need
                                 quantum mechanics? Assume that S is uniform on {0,1}^n. How can A convince C that the samples he sent really came from S and were not handcrafted
                                 by S (if A received classical samples from S)? This seems to us be an almost impossible problem to solve, as we discuss in the paper.
                               </details>
                                <!-- <details><summary>Abstract</summary> Modern machine learning systems have been applied successfully to a variety of tasks in recent years but making such systems robust against adversarially chosen modifications of input
                                  instances seems to be a much harder problem. It is probably fair to say that no fully satisfying solution has been found up to date and it is not clear if the standard formulation even allows for a principled solution.
                                  Hence, rather than following the classical path of bounded perturbations, we consider a model similar to the quantum PAC-learning model introduced by Bshouty and Jackson [1995]. Our first key contribution shows that in
                                  this model we can reduce adversarial robustness to the conjunction of two classical learning theory problems, namely (Problem 1) the problem of finding generative models and (Problem 2) the problem of devising classifiers
                                  that are robust with respect to distributional shifts. Our second key contribution is that the considered framework does not rely on specific (and hence also somewhat arbitrary) threat models like l_p bounded perturbations.
                                  Instead, our reduction guarantees that in order to solve the adversarial robustness problem in our model it suffices to consider a single distance notion, i.e. the Hellinger distance. From the technical perspective our protocols
                                  are heavily based on the recent advances on delegation of quantum computation, e.g. Mahadev [2018].

                                  Although the considered model is quantum and therefore not immediately applicable to ``real-world'' situations, one might hope that in the future either one can find a way to embed ``real-world'' problems into a quantum framework
                                  or that classical algorithms can be found that are capable of mimicking their powerful quantum counterparts.</details> -->
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/2104.05508" target="_blank" style="text-decoration: none"><h4 class="mb-0">Noether: The more things change, the more stay the same</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> </div>
                                <div class="subsubheading mb-0"><b>Under Review</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://arxiv.org/abs/2104.05508" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Informal Abstract</summary>
                                The field of theroetical machine learning managed to produce some initial explanations for why neural networks perform so well in practice.
                                Unfortunately when reading these papers it is hard to see a common theme or a unifying message. In this paper we show how to look at many
                                of the recent advances through one lens, i.e. lens of symmetries. We show how this concept explains many previous results and how it helps to obtain new
                                ones. To do all of that we study a connection to the famous Noether's theorem from physics.</details>
                                <!-- <details><summary>Abstract</summary> Symmetries have proven to be important ingredients in the analysis of neural networks. So far their use has mostly been implicit or seemingly coincidental. We undertake a systematic study of the role that symmetry plays. In particular, we clarify how symmetry interacts with the learning algorithm. The key ingredient in our study is played by Noether's celebrated theorem which, informally speaking, states that symmetry leads to conserved quantities (e.g., conservation of energy or conservation of momentum). In the realm of neural networks under gradient descent, model symmetries imply restrictions on the gradient path. E.g., we show that symmetry of activation functions leads to boundedness of weight matrices, for the specific case of linear activations it leads to balance equations of consecutive layers, data augmentation leads to gradient paths that have "momentum"-type restrictions, and time symmetry leads to a version of the Neural Tangent Kernel. Symmetry alone does not specify the optimization path, but the more symmetries are contained in the model the more restrictions are imposed on the path. Since symmetry also implies over-parametrization, this in effect implies that some part of this over-parametrization is cancelled out by the existence of the conserved quantities. Symmetry can therefore be thought of as one further important tool in understanding the performance of neural networks under gradient descent. </details> -->
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://proceedings.neurips.cc/paper/2021/hash/ae06fbdc519bddaa88aa1b24bace4500-Abstract.html" target="_blank" style="text-decoration: none"><h4 class="mb-0">Exponential Separation between Two Learning Models and Adversarial Robustness</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> </div>
                            <div class="subsubheading mb-0"><b>NeurIPS 2021 (spotlight)</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://proceedings.neurips.cc/paper/2021/hash/ae06fbdc519bddaa88aa1b24bace4500-Abstract.html" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/2102.05475" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Informal Abstract</summary>
                                There are two sides of this paper. The technical one and potential applications to adversarial robustness. On the technical side we show  exponential separation
                                between two models. The standard PAC-learning model and the Equivalence-Query model in which the samples are acquired through an interaction between a teacher
                                and a learner, where the teacher provides counterexamples to hypotheses given by the learner. It is not surprising that one needs fewer samples in the Equivalence-Query
                                model but proving it formally is tricky and the reduction and the proof are quite involved. The second part, i.e. implications for adversarial robustness, was in fact
                                our motivation to study these two models. Our reasoning is as follows. Every adversarial attack in particular provides counterexamples to a given classifier and thus is
                                of the form of the EQ-model. Then the idea is that these adversarial examples can be used for learning faster. This is not a new idea- adversarial training does exactly
                                that. Our contribution is in PROVING that if the adversarial examples have particular structure then they can be used to improve learning over the standard approach.
                                </details>
                                <!-- <details><summary>Abstract</summary> We prove an exponential separation for the sample/query complexity between the standard PAC-learning model and a version of the Equivalence-Query-learning
                                  model. In the PAC model all samples are provided at the beginning of the learning process. In the Equivalence-Query model the samples are acquired through an interaction between a teacher
                                  and a learner, where the teacher provides counterexamples to hypotheses given by the learner. It is intuitive that in an interactive setting fewer samples are needed. We make this formal
                                  and prove that in order to achieve an error epsilon exponentially (in epsilon) fewer samples suffice than what the PAC bound requires. It was shown experimentally by Stutz, Hein, and Schiele
                                  that adversarial training with on-manifold adversarial examples aids generalization (compared to standard training). If we think of the adversarial examples as counterexamples to the current hypothesis
                                  then our result can be thought of as a theoretical confirmation of those findings. We also discuss how our result relates to adversarial robustness. In the standard adversarial model one restricts
                                  the adversary by introducing a norm constraint. An alternative was pioneered by Goldwasser et. al. Rather than restricting the adversary the learner is enhanced. We pursue a third path. We require
                                  the adversary to return samples according to the Equivalance-Query model and show that this leads to robustness. Even though our model has its limitations it provides a fresh point of view on adversarial robustness.</details> -->
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="http://proceedings.mlr.press/v139/gluch21a.html" target="_blank" style="text-decoration: none"><h4 class="mb-0">Query Complexity of Adversarial Attacks</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> </div>
                            <div class="subsubheading mb-0"><b>ICML 2021 </b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="http://proceedings.mlr.press/v139/gluch21a.html" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/2010.01039" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Informal Abstract</summary> How hard is it to adversarially attack a network to which the adversary has black box access only?
                                That, of course, depends on what the adversary knows. If he knew the learning algorithm, training data and the randomness
                                used internally by the algorithm then the model becomes white-box from his point of view. In this paper we analyze what happens
                                if the adversary doesn't know the traning data nor the internal randomness. We show that the number of queries needed ro reliably attack
                                depends on the amount of "entropy" in the decision boundaries. Moreover we prove that some learning algorithms are inherently more secure, e.g. KNN, than
                                others, e.g. linear classifiers.</details>
                                <!-- <details><summary>Abstract</summary> There are two main attack models considered in the adversarial robustness literature: black-box and white-box. We consider these threat models as two ends of a
                                  fine-grained spectrum, indexed by the number of queries the adversary can ask. Using this point of view we investigate how many queries the adversary needs to make to design an attack that is
                                  comparable to the best possible attack in the white-box model. We give a lower bound on that number of queries in terms of entropy of decision boundaries of the classifier. Using this result
                                  we analyze two classical learning algorithms on two synthetic tasks for which we prove meaningful security guarantees. The obtained bounds suggest that some learning algorithms are inherently
                                  more robust against query-bounded adversaries than others.</details> -->
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611976465.97" target="_blank" style="text-decoration: none"><h4 class="mb-0">Spectral Clustering Oracles in Sublinear Time</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://theory.epfl.ch/kapralov/" target="_blank">Michael Kapralov</a>, <a href="https://sites.google.com/site/silviolattanzi/" target="_blank">Silvio Lattanzi</a>,
                            Aida Mousavifar, <a href="https://ls2-www.cs.tu-dortmund.de/grav/en/grav_files/people/sohler" target="_blank">Christian Sohler</a>  </div>
                            <div class="subsubheading mb-0"><b>SODA 2021</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611976465.97" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/2101.05549" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary> Given a graph G that can be partitioned into k disjoint expanders with outer conductance upper bounded by ‚àä ¬´ 1, can we efficiently construct a small space data structure
                                  that allows quickly classifying vertices of G according to the expander (cluster) they belong to? Formally, we would like an efficient local computation algorithm that misclassifies at most an O(epsilon)
                                  fraction of vertices in every expander. We refer to such a data structure as a spectral clustering oracle.

                                  Our main result is a spectral clustering oracle with query time O^*(n^(1/2+O(epsilon))) and preprocessing time 2^O(1/epsilon k^4 log^2(k))n^(1/2 + O(epsilon)) that provides misclassification error O(epsilon log k) per cluster for any epsilon ¬´ 1/log k.
                                  More generally, query time can be reduced at the expense of increasing the preprocessing time appropriately (as long as the product is about n^(1+O(epsilon)) ‚Äì this in particular gives a nearly linear
                                  time spectral clustering primitive.

                                  The main technical contribution is a sublinear time oracle that provides dot product access to the spectral embedding of G by estimating distributions of short random walks from vertices in G.
                                  The distributions themselves provide a poor approximation to the spectral embedding, but we show that an appropriate linear transformation can be used to achieve high precision dot product access.
                                  We give an estimator for this linear transformation and analyze it using spectral perturbation bounds and a novel upper bound on the leverage scores of the spectral embedding matrix of a k-clusterable graph.
                                  We then show that dot product access to the spectral embedding is sufficient to design a clustering oracle. At a high level our approach amounts to hyperplane partitioning in the spectral embedding of G,
                                  but crucially operates on a nested sequence of carefully defined subspaces in the spectral embedding to achieve per cluster recovery guarantees.</details>
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="http://proceedings.mlr.press/v108/gluch20a.html" target="_blank" style="text-decoration: none"><h4 class="mb-0">Constructing a provably adversarially-robust classifier from a high accuracy one</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://people.epfl.ch/rudiger.urbanke" target="_blank">Rudiger Urbanke</a> </div>
                            <div class="subsubheading mb-0"><b>AISTATS 2020</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="http://proceedings.mlr.press/v108/gluch20a.html" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/1912.07561" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>

                                <details><summary>Abstract</summary> Modern machine learning models with very high accuracy have been shown to be vulnerable to small, adversarially chosen perturbations
                                  of the input. Given black-box access to a high-accuracy classifier f, we show how to construct a new classifier g that has high accuracy and is also robust to adversarial
                                  L2-bounded perturbations. Our algorithm builds upon the framework of randomized smoothing that has been recently shown to outperform all previous defenses against L2-bounded adversaries.
                                  Using techniques like random partitions and doubling dimension, we are able to bound the adversarial error of g in terms of the optimum error. In this paper we focus on our conceptual contribution,
                                  but we do present two examples to illustrate our framework. We will argue that, under some assumptions, our bounds are optimal for these cases.</details>
                            </div>
                        </div>
                    </div>

                    <div class="subheading mb-3"> Pre-PhD work </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://drops.dagstuhl.de/opus/volltexte/2019/10317/" target="_blank" style="text-decoration: none"><h4 class="mb-0">The first order truth behind undecidability of regular path queries determinacy</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://ii.uni.wroc.pl/~jma/index.phtml" target="_blank">Jerzy Marcinkowski</a>, <a href="https://ii.uni.wroc.pl/instytut/pracownicy/103" target="_blank">Piotr Ostropolski-Nalewaja</a> </div>
                            <div class="subsubheading mb-0"><b>ICDT 2019</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://drops.dagstuhl.de/opus/volltexte/2019/10317/" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/1808.07767" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary> In our paper [G≈Çuch, Marcinkowski, Ostropolski-Nalewaja, LICS ACM, 2018] we have solved an old problem stated in [Calvanese, De Giacomo, Lenzerini,
                                  Vardi, SPDS ACM, 2000] showing that query determinacy is undecidable for Regular Path Queries. Here a strong generalisation of this result is shown, and --
                                  we think -- a very unexpected one. We prove that no regularity is needed: determinacy remains undecidable even for finite unions of conjunctive path queries.</details>
                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://dl.acm.org/doi/abs/10.1145/3209108.3209120" target="_blank" style="text-decoration: none"><h4 class="mb-0">Can One Escape Red Chains? Regular Path Queries Determinacy is Undecidable</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://ii.uni.wroc.pl/~jma/index.phtml" target="_blank">Jerzy Marcinkowski</a>, <a href="https://ii.uni.wroc.pl/instytut/pracownicy/103" target="_blank">Piotr Ostropolski-Nalewaja</a> </div>
                            <div class="subsubheading mb-0"><b>LICS 2018</b>  <br>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://dl.acm.org/doi/10.1145/3209108.3209120" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">conference</a>
                                    <a href="https://arxiv.org/abs/1802.01554" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract</summary> For a given set of queries (which are expressions in some query language) Q = {Q1, Q2, ... Qk} and for another query Q0 we say that Q determines Q0 if
                                  -- informally speaking -- for every database D, the information contained in the views Q(D) is sufficient to compute Q0(D).

                                  Query Determinacy Problem is the problem of deciding, for given Q and Q0, whether Q determines Q0. Many versions of this problem, for different query languages,
                                  were studied in database theory. In this paper we solve a problem stated in [CGLV02] and show that Query Determinacy Problem is undecidable for the Regular Path Queries
                                  -- the paradigmatic query language of graph databases.</details>

                            </div>
                        </div>
                    </div>

                    <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="resume-content">
                            <a href="https://arxiv.org/abs/1703.01475" target="_blank" style="text-decoration: none"><h4 class="mb-0">4/3 Rectangle Tiling lower bound</h4></a>
                            <div class="subsubheading mb-0">with <a href="https://www.ii.uni.wroc.pl/~lorys/" target="_blank">Krzysztof Lory≈õ</a> </div>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="https://arxiv.org/abs/1703.01475" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">arXiv</a>
                                </div>
                                <details><summary>Abstract </summary> The problem that we consider is the following: given an n√ón array A of positive numbers, find a tiling using at most p rectangles
                                  (which means that each array element must be covered by some rectangle and no two rectangles must overlap) that minimizes the maximum weight of any rectangle
                                  (the weight of a rectangle is the sum of elements which are covered by it). We prove that it is NP-hard to approximate this problem to within a factor of
                                  4/3 (the previous best result was 5/4).</details>

                            </div>
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />
            <!--Teaching-->
            <section class="resume-section" id="teaching">
                <div class="resume-section-content">
                    <h2 class="mb-5">Teaching</h2>
                    <h4 class="mb-0">Videos from a course on "Quantum Interactive Proofs" I held at EPFL can be found
                    <a href="https://kashbrti.github.io/quantum-ip/" target="_blank" class="btn btn-secondary" role="button" aria-pressed="true">here</a></h4>
                    <!--
                    <ul class="list-inline dev-icons">
                        <li class="list-inline-item"><i class="fab fa-html5"></i></li>
                        <li class="list-inline-item"><i class="fab fa-css3-alt"></i></li>
                        <li class="list-inline-item"><i class="fab fa-js-square"></i></li>
                        <li class="list-inline-item"><i class="fab fa-angular"></i></li>
                        <li class="list-inline-item"><i class="fab fa-react"></i></li>
                        <li class="list-inline-item"><i class="fab fa-node-js"></i></li>
                        <li class="list-inline-item"><i class="fab fa-sass"></i></li>
                        <li class="list-inline-item"><i class="fab fa-less"></i></li>
                        <li class="list-inline-item"><i class="fab fa-wordpress"></i></li>
                        <li class="list-inline-item"><i class="fab fa-gulp"></i></li>
                        <li class="list-inline-item"><i class="fab fa-grunt"></i></li>
                        <li class="list-inline-item"><i class="fab fa-npm"></i></li>
                    </ul>
                    <div class="subheading mb-3">Workflow</div>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Mobile-First, Responsive Design
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cross Browser Testing & Debugging
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cross Functional Teams
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Agile Development & Scrum
                        </li>
                    </ul>
                  -->
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="education">
               <div class="resume-section-content">
                   <h2 class="mb-5">Education</h2>
                   <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                       <div class="flex-grow-1">
                           <h3 class="mb-0">√âcole Polytechnique F√©d√©rale de Lausanne (<span style="font-weight: bold; color: #ff0000;">EPFL</span>)</h3>
                           <div>Doctor of Philosophy</div>
                           <div class="subheading mb-3">Computer Science</div>
                           <!-- <p>GPA: */6 </p> -->
                       </div>
                       <div class="flex-shrink-0"><span class="text-primary">2018 - 2024</span></div>
                   </div>
                   <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                       <div class="flex-grow-1">
                           <h3 class="mb-0">University of Wroclaw</h3>
                           <div>Master of Science</div>
                           <div class="subheading mb-3">Computer Science</div>
                           <!-- <p>GPA: */6 </p> -->
                           <ul>
                               <li>MSc Research Scholar, working with <a href="https://ii.uni.wroc.pl/~jma/index.phtml" target="_blank">Jerzy Marcinkowski</a></li>
                           </ul>
                       </div>
                       <div class="flex-shrink-0"><span class="text-primary">2015 - 2018</span></div>
                   </div>
                   <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                       <div class="flex-grow-1">
                           <h3 class="mb-0">University of Wroclaw</h3>
                           <div>Bachelor of Science</div>
                           <div class="subheading mb-3">Mathematics</div>
                           <!-- <p>GPA: */6 </p> -->
                       </div>
                       <div class="flex-shrink-0"><span class="text-primary">2012 - 2015</span></div>
                   </div>
                   <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                       <div class="flex-grow-1">
                           <h3 class="mb-0">University of Wroclaw</h3>
                           <div>Bachelor of Science</div>
                           <div class="subheading mb-3">Computer Science</div>
                           <!-- <p>GPA: */6 </p> -->
                       </div>
                       <div class="flex-shrink-0"><span class="text-primary">2012 - 2015</span></div>
                   </div>
               </div>
           </section>
            <hr class="m-0" />
            <!-- Skills-->
            <!--
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <div class="subheading mb-3">Programming Languages & Tools</div>
                    <ul class="list-inline dev-icons">
                        <li class="list-inline-item"><i class="fab fa-html5"></i></li>
                        <li class="list-inline-item"><i class="fab fa-css3-alt"></i></li>
                        <li class="list-inline-item"><i class="fab fa-js-square"></i></li>
                        <li class="list-inline-item"><i class="fab fa-angular"></i></li>
                        <li class="list-inline-item"><i class="fab fa-react"></i></li>
                        <li class="list-inline-item"><i class="fab fa-node-js"></i></li>
                        <li class="list-inline-item"><i class="fab fa-sass"></i></li>
                        <li class="list-inline-item"><i class="fab fa-less"></i></li>
                        <li class="list-inline-item"><i class="fab fa-wordpress"></i></li>
                        <li class="list-inline-item"><i class="fab fa-gulp"></i></li>
                        <li class="list-inline-item"><i class="fab fa-grunt"></i></li>
                        <li class="list-inline-item"><i class="fab fa-npm"></i></li>
                    </ul>
                    <div class="subheading mb-3">Workflow</div>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Mobile-First, Responsive Design
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cross Browser Testing & Debugging
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cross Functional Teams
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Agile Development & Scrum
                        </li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />
          -->
            <!-- Interests-->
            <!--
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>Apart from being a web developer, I enjoy most of my time being outdoors. In the winter, I am an avid skier and novice ice climber. During the warmer months here in Colorado, I enjoy mountain biking, free climbing, and kayaking.</p>
                    <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements in the front-end web development world.</p>
                </div>
            </section>
            <hr class="m-0" />
            -->
            <!-- Awards-->
            <!--
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Google Analytics Certified Developer
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Mobile Web Specialist - Google Certification
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2009
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Adobe Creative Jam 2008 (UI Design Category)
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2008
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - James Buchanan High School - Hackathon 2006
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - James Buchanan High School - Hackathon 2005
                        </li>
                    </ul>
                </div>
            </section>
            -->
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
